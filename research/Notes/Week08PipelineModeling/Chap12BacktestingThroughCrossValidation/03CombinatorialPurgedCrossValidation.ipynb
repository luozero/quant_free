{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d99e9c3dd769b4",
   "metadata": {},
   "source": [
    "### Combinatorial Purged Cross Validation\n",
    "\n",
    "In this section, we introduce a new method that addresses the main drawbacks of walk-forward and cross-validation techniques. Professor Prado calls this method Combinatorial Purged Cross-Validation (CPCV). Given the number of backtest paths $\\varphi$ that the researcher aims for, CPCV generates the exact combinations of test/training sets needed to create these paths, while simultaneously removing observations with leaked information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:20:34.848550Z",
     "start_time": "2024-08-10T15:20:34.845309Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c7f962b0ae2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quant_free.finml.labeling.labeling import *\n",
    "\n",
    "triple_barrier_event = pd.read_parquet('./research/Data/AAPL_triple_barrier_events.parquet')\n",
    "avg_uniqueness = pd.read_parquet('./research/Data/AAPL_avg_unique.parquet')\n",
    "feature_matrix = pd.read_parquet('./research/Data/AAPL_feature_matrix.parquet')\n",
    "\n",
    "labels = meta_labeling(\n",
    "    triple_barrier_event, \n",
    "    feature_matrix['Close']\n",
    ")\n",
    "triple_barrier_event['side'] = labels['bin']\n",
    "meta_labels = meta_labeling(\n",
    "    triple_barrier_event, # with side labels\n",
    "    feature_matrix['Close']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9ff7cecce2462",
   "metadata": {},
   "source": [
    "#### 1. 조합적 분할\n",
    "\n",
    "Let's consider the case of partitioning $ T $ observations into $ N $ groups without shuffling. In this scenario, the size of groups $ n = 1, \\dots, N-1 $ is $\\left[ \\frac{T}{N} \\right]$, while the size of the $ N $-th group is $ T - \\left[ \\frac{T}{N} \\right] (N-1) $, where $[.]$ denotes the floor or integer function. \n",
    "\n",
    "For a test set of size $ k $, the number of possible training/test splits is given by:\n",
    "\n",
    "$$\n",
    "{N \\choose N-k} = \\frac{\\prod_{i=0}^{k-1} \\left( N-i\\right)}{k!}\n",
    "$$\n",
    "\n",
    "Each combination contains $ k $ tested groups, so the total number of tested groups is $ k {N \\choose N-k} $. Since all combinations are calculated, these groups are uniformly distributed across all $ N $. This implies that we can perform a total of $ \\varphi \\left[ N, k\\right] $ backtests for a test set of size $ k $ from $ N $ groups:\n",
    "\n",
    "$$\n",
    "\\varphi \\left[ N, k\\right] = \\frac{k}{N} {N \\choose N-k} = \\frac{\\prod_{i=0}^{k-1} \\left( N-i\\right)}{(k-1)!}\n",
    "$$\n",
    "\n",
    "The figure below illustrates the configuration of training/test splits for $ N = 6 $ and $ k=2 $. There are ${6 \\choose 4} = 15$ splits, indexed as $ S1, \\dots, S15 $. The groups marked with an \"x\" belong to the test set, while those without markings belong to the training set. Each group contributes to forming $ \\varphi[6, 2] = 5 $ test sets, thus the training/test splitting method generates 5 backtest paths.\n",
    "\n",
    "![1](images/1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856604233396fb4f",
   "metadata": {},
   "source": [
    "The figure below illustrates the allocation of a single backtest path to each tested group. For example, Path 1 combines predictions from groups $ (G1, S1), (G2, S1), (G3, S2), (G4, S3), (G5, S4), (G6, S5) $, while Path 2 combines predictions from $ (G1, S2), (G2, S6), (G3, S6), (G4, S7), (G5, S8), (G6, S9) $.\n",
    "\n",
    "\n",
    "![2](images/2.png) \n",
    "\n",
    "\n",
    "This represents a path generated by training the classifier on the data portion defined by $ \\theta = \\frac{1-k}{N} $. Although theoretically, learning is possible when $ \\theta < \\frac{1}{2} $, in practice, it is assumed that $ k \\leq \\frac{K}{2} $. The data portion of the training set, $ \\theta $, increases as $ N \\rightarrow T $ but decreases as $ k \\rightarrow \\frac{N}{2} $. The number of paths $ \\varphi \\left[ N, k\\right] $ increases with $ N \\rightarrow T $ and $ k \\rightarrow \\frac{N}{2} $. In the limit, the maximum number of paths can be achieved by setting $ N = T $ and $ k = \\frac{N}{2} = \\frac{T}{2} $, at the cost of using only half of the data for training in each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56355da658fb361c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:49:53.094615Z",
     "start_time": "2024-08-10T15:49:53.083871Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix['side'] = triple_barrier_event['side'].copy()\n",
    "feature_matrix['label'] = meta_labels['bin'].copy()\n",
    "feature_matrix.drop(['Open','High','Low','Close','Adj Close','Volume'], axis = 1, inplace = True)\n",
    "feature_matrix.dropna(inplace = True)\n",
    "matrix = feature_matrix[feature_matrix['side'] != 0]\n",
    "\n",
    "X = matrix.drop(['side','label'], axis = 1)\n",
    "y = matrix['label']\n",
    "\n",
    "X_train, X_test = X.loc[:'2019'], X.loc['2020':]\n",
    "y_train, y_test = y.loc[:'2019'], y.loc['2020':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ce1b1cb1268107e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:51:34.440170Z",
     "start_time": "2024-08-10T15:51:34.434781Z"
    }
   },
   "outputs": [],
   "source": [
    "samples_info_sets = triple_barrier_event.loc[X_train.index].loc[:'2019', 't1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10305bac80311a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T15:51:51.227872Z",
     "start_time": "2024-08-10T15:51:47.115119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8866\n",
      "Accuracy: 0.8786\n",
      "Accuracy: 0.8618\n",
      "Accuracy: 0.8769\n",
      "Accuracy: 0.9042\n",
      "Accuracy: 0.9010\n",
      "Accuracy: 0.9201\n",
      "Accuracy: 0.9002\n",
      "Accuracy: 0.9129\n",
      "Accuracy: 0.9041\n"
     ]
    }
   ],
   "source": [
    "from quant_free.finml.cross_validation.combinatorial import CombinatorialPurgedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "comb_purge_fold = CombinatorialPurgedKFold(\n",
    "    n_splits = 5, \n",
    "    n_test_splits = 2, \n",
    "    samples_info_sets = samples_info_sets, \n",
    "    pct_embargo = 0.01\n",
    ")\n",
    "\n",
    "for train_indices, test_indices in comb_purge_fold.split(X_train, y_train):\n",
    "    X_train_valid, X_test_valid = X_train.iloc[train_indices], X_train.iloc[test_indices]\n",
    "    y_train_valid, y_test_valid = y_train.iloc[train_indices], y_train.iloc[test_indices]\n",
    "\n",
    "    clf = RandomForestClassifier(random_state = 42)\n",
    "    clf.fit(X_train_valid, y_train_valid)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_valid)\n",
    "    accuracy = accuracy_score(y_test_valid, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
